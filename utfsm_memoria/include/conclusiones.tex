\chapter*{Conclusiones}
\addcontentsline{toc}{chapter}{Conclusiones}

%Colocar los principales descubrimientos (aunque se repitan)

Las proteínas constituyen una parte fundamental de la biología actual, son partícipes de numerosas funciones vitales en los seres vivos, al día de hoy se siguen investigando sobre nuevos polipéptidos que van apareciendo y también se siguen buscando cuáles son los aminoácidos y/o secuencias responsables directamente de estas funciones vitales. Estas proteínas han sido registradas y guardadas en archivos \textit{.fasta} como cadenas de strings, y pueden ser descargadas de manera gratuita en las páginas encargadas, como UniProt y EROP-Moscow.

Lo que se puso como objetivo en este trabajo es desarrollar en base a estructuras de indexación (Indexed Text Searching) una manera entendible y conveniente para obtener los diferentes fragmentos de proteínas de largo entre 1 hasta 50 aminoácidos, y a partir de aquellos extraer las secuencias que más se repetían. Y mirando los resultados logrados en la sección anterior, se puede concluir que el uso del \textbf{arreglo de sufijos}, \textbf{arreglo LCP} y \textbf{\textit{priority queue}} para este trabajo permitió lograr con éxito esta tarea.

\subsubsection{Problemas presentados en el desarrollo de la implementación}

Aunque en el desarrollo de la implementación, aparecieron variados problemas a la hora de trabajar ciertos aspectos. Primero fue el hecho \textbf{de sustraer las cadenas de proteínas de los archivos .fasta y alojarlas en otro archivo encadenándolas en un solo string}, donde el tiempo empleado para realizar esto dependía del tamaño del archivo analizado.\\
Un segundo problema fue que la cadena de strings obtenidas de la base de datos de UniProt-TrEMBL era muy grande para formar el vector correspondiente al arreglo de sufijos, lo cual originó \textbf{la búsqueda de una nueva alternativa de implementación} de esta estructura en conjunto con su arreglo LCP (algoritmos en memoria externa). Este hecho permitió comparar resultados logrados con los \textit{datasets} usados y de esa manera usar esta ``comparación'' como ejemplo de veracidad de los resultados obtenidos, en especial para la sección de los residuos que más se repiten.\\
El tercer problema fue el hecho de identificar en los archivos \textbf{cuáles eran los caracteres prohibidos que no se debían tomar en cuenta} para desarrollar este problema. Ante eso las bases de datos pertenecientes a UniProt (SwissProt-TrEMBL-Homosapiens) tenían diferentes letras prohibidas que la base de datos de EROP-Moscow (este archivo tenía cadenas de oligopéptidos en los que se encontraban cadenas de radicales polarizados en los extremos colocados con los signos $+$ (positivo) y $-$ (negativo)). Estos detalles hicieron modificar ciertas líneas del archivo principal agregando más letras prohibidas a revisar. En consecuencia supuso un aumento (que no fue significativo) en los tiempos al obtener los resultados finales.

\subsubsection{Extensión del algoritmo a otros temas}

Si bien el algoritmo fue desarrollado con el objetivo de hallar los diferentes fragmentos de secuencias de proteínas y sus repeticiones, también se puede tomar y modificar este algoritmo para trabajar en:

\begin{enumerate}

\item Cadenas de ADN (diferentes substrings de tamaño $k$ y cuáles son los que más se repiten).
\item Todo tipo de textos, ya sean científicos, literarios, de entretención, etc. (buscar cantidad de vocales y/o trozos diferentes de palabras de determinado tamaño que hay en un determinado texto).

\end{enumerate}

Con respecto a la opción de cadenas de ADN, existen variados genomas disponibles a descargar los cuales también se encuentran en formato .fasta. Por lo mismo manipularlos a la hora de extraer la cadena tendría el mismo procedimiento que la obtención de la cadena de proteínas, incluso es más sencilla ya que solamente se trabajaría con una cadena genérica de ADN, no habría concatenación con otras cadenas, por otra parte acá se tendrían 4 nucleótidos a revisar y la combinatoria de potenciales substrings de tamaño $k$ a encontrar sería mucho más pequeña.\\
Por el lado de los textos sería más difícil extraer una cadena de palabras, se tendrían que eliminar los espacios o considerarlos como carácteres prohibidos a igual que los elementos paraverbales, como los signos de interrogación, signos de exclamación, puntos, etc. Por otro lado, el universo de letras a considerar serían 27 (las letras del abecedario), de manera que la combinatoria de posibles secuencias de palabras sería bastante más grande que con las proteínas. Lo positivo de trabajar con estos textos es que se utilizan \textbf{caracteres ordenados en ``palabras''} por lo cual aplicando el algoritmo a estos textos y obteniendo resultados permitiría estudiar el tipo de lenguaje o expresiones que usa un determinado autor para escribir su obra.

\subsubsection{Posible trabajo a futuro}

La solución entregada en esta memoria con respecto al problema analizado de ninguna manera puede ser catalogado como definitivo, obviamente podrían haber mejores propuestas de implementaciones. El real potencial de esto es el \textbf{uso del arreglo de sufijos en una aplicación} demostrando que su utilización es muy importante para trabajar cadenas que para este caso fueron las proteínas. En el caso de los tiempos logrados pueden ser más pequeños si se utilizará un servidor más potente (el servidor de la Universidad es bastante completo a nivel de velocidad y memoria RAM) o mejorar ciertos pasos de la implementación, por ejemplo cuando \textbf{se pasan los datos de los archivos .sa y .lcp a los archivos manipulables .txt para usarlos en el programa principal}. Sería bueno intentar extraer toda esa información sin crear un archivo auxiliar, lo que no solo impondría una mejora notable en los tiempos, sino que también se ocuparía mucho menos espacio en el disco duro.

En lo relativo a investigación, actualmente se está analizando los diferentes substrings de tamaño $k$ y aquellos que más se repiten de ciertos organismos (animales, plantas, bacterias, entre otros) extraídos de las bases de datos de UniProt (SwissProt y TrEMBL) y EROP-Moscow, de los cuáles se realizarán interpretaciones biológicas y fisicoquímicas de los resultados obtenidos.

En definitiva siempre será relevante trabajar con proteínas, a cada día, a cada minuto se va descubriendo una nueva proteína, una nueva función correspondiente a un determinado aminoácido, posiblemente se lleguen a descubrir cientos de péptidos en el futuro ya que aún quedan muchas cosas por resolver con respecto a las propiedades de estas estructuras.

Y con respecto a las estructuras de indexación, sus avances y motivación de trabajo han ido a una velocidad tan rápida que a casi 20 años de su creación ya ocupan un sector importante para analizar cadenas de strings. Esto ha ido de la mano directamente con el crecimiento de las tecnologías de información (TI).